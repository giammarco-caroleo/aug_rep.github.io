## A Cross-Task Visuo-Tactile Representation Using Point Clouds

# Abstract 
The combination of visual and tactile cues has proved to be effective for object recognition as well as grasping and manipulation tasks. Nevertheless, the two modalities are not trivial to combine since tactile and visual data carry distinct information and may be structurally different. Researchers have addressed this problem by proposing approaches that either do not consider mechanical properties conveyed by tactile sensors or cannot be deployed directly for diverse tasks. In this paper, we propose a cross-task visuo-tactile representation that encodes both the geometrical and mechanical properties of objects in a point cloud data structure. By physically exploring different areas of a given item, we collect tactile information to estimate the local compliance of the surface, encoding it as the color information of the point cloud in the probed areas. Then, this color information is extended to the entire object assuming that neighboring points share the same mechanical properties. We apply the proposed point cloud to a set of 6 real-world objects showing that it  can be effectively used to encode the shape of the objects along with their information on the local compliance. Further, we show that the augmented point cloud can be used for different tasks by exploiting this in three robotic tasks - a visuo-tactile object classification problem, a path following and a reaching in clutter tasks.
